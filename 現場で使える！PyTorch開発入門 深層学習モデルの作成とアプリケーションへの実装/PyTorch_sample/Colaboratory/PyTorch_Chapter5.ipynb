{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_Chapter5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "NhdPuZ50Z3jW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Chapter5　自然言語処理と回帰型ニューラルネットワーク\n"
      ]
    },
    {
      "metadata": {
        "id": "2J_GU7iqJVNw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qh7fZMTMUItp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.4.0-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "!pip3 install tqdm\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3QuNFp23Y42z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.tensor([1,2,3]).to(\"cuda:0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7918RVvwAZkQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.1　全部で10000種類のトークンを20次元のベクトルで表現する場合"
      ]
    },
    {
      "metadata": {
        "id": "wxikaWg3AXwg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "emb = nn.Embedding(10000, 20, padding_idx=0)\n",
        "# Embedding層への入力はint64のTensor\n",
        "inp = torch.tensor([1, 2, 5, 2, 10], dtype=torch.int64)\n",
        "# 出力はfloat32のTensor\n",
        "out = emb(inp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FIEOf1yKAkvw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Colaboratory における圧縮ファイルの展開"
      ]
    },
    {
      "metadata": {
        "id": "Xrr8eipPUTc5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar xf aclImdb_v1.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_i7tjjGDDk7Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "ファイルの確認"
      ]
    },
    {
      "metadata": {
        "id": "3RwT5eetZf9F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JhobXq6nZq_N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls aclImdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_U5ciwYoaEHW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls aclImdb/test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H37IUR8iaLH-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls aclImdb/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B-dtb4_HjEDQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.2　関数の作成"
      ]
    },
    {
      "metadata": {
        "id": "jpYGSSxyXiQj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pathlib\n",
        "import re\n",
        "\n",
        "remove_marks_regex = re.compile(\"[,\\.\\(\\)\\[\\]\\*:;]|<.*?>\")\n",
        "shift_marks_regex = re.compile(\"([?!])\")\n",
        "\n",
        "def text2ids(text, vocab_dict):\n",
        "    # !?以外の記号の削除\n",
        "    text = remove_marks_regex.sub(\"\", text)\n",
        "    # !?と単語の間にスペースを挿入\n",
        "    text = shift_marks_regex.sub(r\" \\1 \", text)\n",
        "    tokens = text.split()\n",
        "    return [vocab_dict.get(token, 0) for token in tokens]\n",
        "\n",
        "def list2tensor(token_idxes, max_len=100, padding=True):\n",
        "    if len(token_idxes) > max_len:\n",
        "        token_idxes = token_idxes[:max_len]\n",
        "    n_tokens = len(token_idxes)\n",
        "    if padding:\n",
        "        token_idxes = token_idxes \\\n",
        "            + [0] * (max_len - len(token_idxes))\n",
        "    return torch.tensor(token_idxes, dtype=torch.int64), n_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "isLoIwvLjTrU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.3　Datasetクラスの作成"
      ]
    },
    {
      "metadata": {
        "id": "Fe-yQzZ0bHD2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import (Dataset, \n",
        "                              DataLoader,\n",
        "                              TensorDataset)\n",
        "import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jIXweJEtXsG4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, dir_path, train=True,\n",
        "                 max_len=100, padding=True):\n",
        "        self.max_len = max_len\n",
        "        self.padding = padding\n",
        "        \n",
        "        path = pathlib.Path(dir_path)\n",
        "        vocab_path = path.joinpath(\"imdb.vocab\")\n",
        "        \n",
        "        # ボキャブラリファイルを読み込み、行ごとに分割\n",
        "        self.vocab_array = vocab_path.open() \\\n",
        "                            .read().strip().splitlines()\n",
        "        # 単語をキーとし、値がIDのdictを作る\n",
        "        self.vocab_dict = dict((w, i+1) \\\n",
        "            for (i, w) in enumerate(self.vocab_array))\n",
        "    \n",
        "        if train:\n",
        "            target_path = path.joinpath(\"train\")\n",
        "        else:\n",
        "            target_path = path.joinpath(\"test\")\n",
        "        pos_files = sorted(glob.glob(\n",
        "            str(target_path.joinpath(\"pos/*.txt\"))))\n",
        "        neg_files = sorted(glob.glob(\n",
        "            str(target_path.joinpath(\"neg/*.txt\"))))\n",
        "        # posは1, negは0のlabelを付けて\n",
        "        # (file_path, label)のtupleのリストを作成\n",
        "        self.labeled_files = \\\n",
        "            list(zip([0]*len(neg_files), neg_files )) + \\\n",
        "            list(zip([1]*len(pos_files), pos_files))\n",
        "  \n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab_array)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labeled_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label, f = self.labeled_files[idx]\n",
        "        # ファイルのテキストデータを読み取って小文字に変換\n",
        "        data = open(f).read().lower()\n",
        "        # テキストデータをIDのリストに変換\n",
        "        data = text2ids(data, self.vocab_dict)\n",
        "        # IDのリストをTensorに変換\n",
        "        data, n_tokens = list2tensor(data, self.max_len, self.padding)\n",
        "        return data, label, n_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SnpOhI17jrX1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.4　訓練用とテスト用のDataLoaderの作成（your_path>を変更している）"
      ]
    },
    {
      "metadata": {
        "id": "nLFBANMoXw44",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = IMDBDataset(\"aclImdb/\")\n",
        "test_data = IMDBDataset(\"aclImdb/\", train=False)\n",
        "train_loader = DataLoader(train_data, batch_size=32,\n",
        "                          shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_data, batch_size=32,\n",
        "                        shuffle=False, num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d6bcdixyjxp9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.5　ネットワークの定義"
      ]
    },
    {
      "metadata": {
        "id": "Fv_mZgiYZZcs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SequenceTaggingNet(nn.Module):\n",
        "    def __init__(self, num_embeddings,\n",
        "                 embedding_dim=50, \n",
        "                 hidden_size=50,\n",
        "                 num_layers=1,\n",
        "                 dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(num_embeddings, embedding_dim,\n",
        "                                padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                            hidden_size, num_layers,\n",
        "                            batch_first=True, dropout=dropout)\n",
        "        self.linear = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x, h0=None, l=None):\n",
        "        # IDをEmbeddingで多次元のベクトルに変換する\n",
        "        # xは(batch_size, step_size) \n",
        "        # -> (batch_size, step_size, embedding_dim)\n",
        "        x = self.emb(x)\n",
        "        # 初期状態h0と共にRNNにxを渡す\n",
        "        # xは(batch_size, step_size, embedding_dim)\n",
        "        # -> (batch_size, step_size, hidden_dim)\n",
        "        x, h = self.lstm(x, h0)\n",
        "        # 最後のステップのみ取り出す\n",
        "        # xは(batch_size, step_size, hidden_dim)\n",
        "        # -> (batch_size, 1)\n",
        "        if l is not None:\n",
        "            # 入力のもともとの長さがある場合はそれを使用する\n",
        "            x = x[list(range(len(x))), l-1, :]\n",
        "        else:\n",
        "            # なければ単純に最後を使用する\n",
        "            x = x[:, -1, :]\n",
        "        # 取り出した最後のステップを線形層に入れる\n",
        "        x = self.linear(x)\n",
        "        # 余分な次元を削除する\n",
        "        # (batch_size, 1) -> (batch_size, )\n",
        "        x = x.squeeze()\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ULc8DkQMlh64",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.6　訓練の作成"
      ]
    },
    {
      "metadata": {
        "id": "Ic1X_2y5a_w1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def eval_net(net, data_loader, device=\"cpu\"):\n",
        "    net.eval()\n",
        "    ys = []\n",
        "    ypreds = []\n",
        "    for x, y, l in data_loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        l = l.to(device)\n",
        "        with torch.no_grad():\n",
        "            y_pred = net(x, l=l)\n",
        "            y_pred = (y_pred > 0).long()\n",
        "            ys.append(y)\n",
        "            ypreds.append(y_pred)\n",
        "    ys = torch.cat(ys)\n",
        "    ypreds = torch.cat(ypreds)\n",
        "    acc = (ys == ypreds).float().sum() / len(ys)\n",
        "    return acc.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R3td3RShlqvx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.7　評価の作成"
      ]
    },
    {
      "metadata": {
        "id": "2UZpd1DwcBuD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "\n",
        "# num_embeddingsには0を含めてtrain_data.vocab_size+1を入れる\n",
        "net = SequenceTaggingNet(train_data.vocab_size+1, num_layers=2)\n",
        "net.to(\"cuda:0\")\n",
        "opt = optim.Adam(net.parameters())\n",
        "loss_f = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in range(10):\n",
        "    losses = []\n",
        "    net.train()\n",
        "    for x, y, l in tqdm.tqdm(train_loader):\n",
        "        x = x.to(\"cuda:0\")\n",
        "        y = y.to(\"cuda:0\")\n",
        "        l = l.to(\"cuda:0\")\n",
        "        y_pred = net(x, l=l)\n",
        "        loss = loss_f(y_pred, y.float())\n",
        "        net.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        losses.append(loss.item())\n",
        "    train_acc = eval_net(net, train_loader, \"cuda:0\")\n",
        "    val_acc = eval_net(net, test_loader, \"cuda:0\")\n",
        "    print(epoch, mean(losses), train_acc, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dT45FkrXl1KF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.8　RNNを使用しないモデルの作成（<your_path>を変更している）"
      ]
    },
    {
      "metadata": {
        "id": "GgPjfCXTcZVj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_svmlight_file\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "train_X, train_y = load_svmlight_file(\n",
        "    \"aclImdb/train/labeledBow.feat\")\n",
        "test_X, test_y = load_svmlight_file(\n",
        "    \"aclImdb/test/labeledBow.feat\",\n",
        "    n_features=train_X.shape[1])\n",
        "\n",
        "model = LogisticRegression(C=0.1, max_iter=1000)\n",
        "model.fit(train_X, train_y)\n",
        "model.score(train_X, train_y), model.score(test_X, test_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LfZqtG3fl5hI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.9　PackedSequenceの性質を利用したモデルの作成"
      ]
    },
    {
      "metadata": {
        "id": "-TA67QRjc7Aj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SequenceTaggingNet2(SequenceTaggingNet):\n",
        "\n",
        "    def forward(self, x, h0=None, l=None):\n",
        "        # IDをEmbeddingで多次元のベクトルに変換\n",
        "        x = self.emb(x)\n",
        "        \n",
        "        # 長さ情報が与えられている場合はPackedSequenceを作る\n",
        "        if l is not None:\n",
        "            x = nn.utils.rnn.pack_padded_sequence(\n",
        "                x, l, batch_first=True)\n",
        "        \n",
        "        # RNNに通す\n",
        "        x, h = self.lstm(x, h0)\n",
        "        \n",
        "        # 最後のステップを取り出して線形層に入れる\n",
        "        if l is not None:\n",
        "            # 長さ情報がある場合は最後の層の\n",
        "            # 内部状態のベクトルを直接利用できる\n",
        "            # LSTMは通常の内部状態の他にブロックセルの状態も\n",
        "            # あるので内部状態のみを使用する\n",
        "            hidden_state, cell_state = h\n",
        "            x = hidden_state[-1]\n",
        "        else:\n",
        "            x = x[:, -1, :]\n",
        "            \n",
        "        # 線形層に入れる\n",
        "        x = self.linear(x).squeeze()\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EogHLqYDmQD5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.10　訓練部の作成"
      ]
    },
    {
      "metadata": {
        "id": "XBrYlXp-de-M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    losses = []\n",
        "    net.train()\n",
        "    for x, y, l in tqdm.tqdm(train_loader):\n",
        "        # 長さの配列を長い順にソート\n",
        "        l, sort_idx = torch.sort(l, descending=True)\n",
        "        # 得られたインデクスを使用してx,yも並べ替え\n",
        "        x = x[sort_idx]\n",
        "        y = y[sort_idx]\n",
        "        \n",
        "        x = x.to(\"cuda:0\")\n",
        "        y = y.to(\"cuda:0\")\n",
        "        \n",
        "        y_pred = net(x, l=l)\n",
        "        loss = loss_f(y_pred, y.float())\n",
        "        net.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        losses.append(loss.item())\n",
        "    train_acc = eval_net(net, train_loader, \"cuda:0\")\n",
        "    val_acc = eval_net(net, test_loader, \"cuda:0\")\n",
        "    print(epoch, mean(losses), train_acc, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mJuVRdizmfdp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.11　語彙辞書と2つの変換関数の作成"
      ]
    },
    {
      "metadata": {
        "id": "5p5a0EU1fXEc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# すべてのascii文字で辞書を作る\n",
        "import string\n",
        "all_chars = string.printable\n",
        "vocab_size = len(all_chars)\n",
        "vocab_dict = dict((c, i) for (i, c) in enumerate(all_chars))\n",
        "\n",
        "# 文字列を数値のリストに変換する関数\n",
        "def str2ints(s, vocab_dict):\n",
        "    return [vocab_dict[c] for c in s]\n",
        "\n",
        "# 数値のリストを文字列に変換する関数\n",
        "def ints2str(x, vocab_array):\n",
        "    return \"\".join([vocab_array[i] for i in x])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vQYyg8uLBxaD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Colaboratory におけるファイルのアップロード"
      ]
    },
    {
      "metadata": {
        "id": "lCx_4mbkfrg7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# ダイアログが表示され、ローカルのファイルを選択してアップロード\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EQAaIsynmp7q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.12　分割するDatasetクラスの定義"
      ]
    },
    {
      "metadata": {
        "id": "OboSZrWEbdw3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import (Dataset, \n",
        "                              DataLoader,\n",
        "                              TensorDataset)\n",
        "import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tYDAtefgjR2u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, path, chunk_size=200):\n",
        "        # ファイルを読み込み、数値のリストに変換する\n",
        "        data = str2ints(open(path).read().strip(), vocab_dict)\n",
        "        \n",
        "        # Tensorに変換し、splitする\n",
        "        data = torch.tensor(data, dtype=torch.int64).split(chunk_size)\n",
        "        \n",
        "        # 最後のchunkの長さをチェックして足りない場合には捨てる\n",
        "        if len(data[-1]) < chunk_size:\n",
        "            data = data[:-1]\n",
        "            \n",
        "        self.data = data\n",
        "        self.n_chunks = len(self.data)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.n_chunks\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1tvJiXCBm8t_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.13　Datasetクラスを使用して、DataLoaderまでを作成（<your_path>を変更している）"
      ]
    },
    {
      "metadata": {
        "id": "cPyQqXNhkDIe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ds = ShakespeareDataset(\"tinyshakespeare.txt\", chunk_size=200)\n",
        "loader = DataLoader(ds, batch_size=32, shuffle=True, num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "69DeHD_pnEB6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.14　文章生成のモデル構築"
      ]
    },
    {
      "metadata": {
        "id": "E7hfZUVfk1fe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SequenceGenerationNet(nn.Module):\n",
        "    def __init__(self, num_embeddings, \n",
        "                 embedding_dim=50, \n",
        "                 hidden_size=50,\n",
        "                 num_layers=1, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, \n",
        "                            hidden_size,\n",
        "                            num_layers,\n",
        "                            batch_first=True,\n",
        "                            dropout=dropout)\n",
        "        # Linerのoutputのサイズは最初のEmbeddingの\n",
        "        # inputサイズと同じnum_embeddings\n",
        "        self.linear = nn.Linear(hidden_size, num_embeddings)\n",
        "\n",
        "    def forward(self, x, h0=None):\n",
        "        x = self.emb(x)\n",
        "        x, h = self.lstm(x, h0)\n",
        "        x = self.linear(x)\n",
        "        return x, h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SX1vD5bYnJTU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.15　文章を生成する関数の作成"
      ]
    },
    {
      "metadata": {
        "id": "Ciy547IsmhU4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_seq(net, start_phrase=\"The King said \",\n",
        "                 length=200, temperature=0.8, device=\"cpu\"):\n",
        "    # モデルを評価モードにする\n",
        "    net.eval()\n",
        "    # 出力の数値を格納するリスト\n",
        "    result = []\n",
        "    \n",
        "    # 開始文字列をTensorに変換\n",
        "    start_tensor = torch.tensor(\n",
        "        str2ints(start_phrase, vocab_dict),\n",
        "        dtype=torch.int64\n",
        "    ).to(device)\n",
        "    # 先頭にbatch次元を付ける\n",
        "    x0 = start_tensor.unsqueeze(0) \n",
        "    # RNNに通して出力と新しい内部状態を得る\n",
        "    o, h = net(x0)\n",
        "    # 出力を(正規化されていない)確率に変換\n",
        "    out_dist = o[:, -1].view(-1).exp()\n",
        "    # 確率から実際の文字のインデクスをサンプリング\n",
        "    top_i = torch.multinomial(out_dist, 1)[0]\n",
        "    # 結果を保存\n",
        "    result.append(top_i)\n",
        "    \n",
        "    # 生成された結果を次々にRNNに入力していく\n",
        "    for i in range(length):\n",
        "        inp = torch.tensor([[top_i]], dtype=torch.int64)\n",
        "        inp = inp.to(device)\n",
        "        o, h = net(inp, h)\n",
        "        out_dist = o.view(-1).exp()\n",
        "        top_i = torch.multinomial(out_dist, 1)[0]\n",
        "        result.append(top_i)\n",
        "        \n",
        "    # 開始文字列と生成された文字列をまとめて返す\n",
        "    return start_phrase + ints2str(result, all_chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dY2oxrECnWHk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.16　文章を生成する関数の作成"
      ]
    },
    {
      "metadata": {
        "id": "fFQ_LNA3mBKP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "\n",
        "net = SequenceGenerationNet(vocab_size, 20, 50,\n",
        "                            num_layers=2, dropout=0.1)\n",
        "net.to(\"cuda:0\")\n",
        "opt = optim.Adam(net.parameters())\n",
        "# 多クラスの識別で問題なのでSoftmaxCrossEntropyLossが損失関数となる\n",
        "loss_f = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(50):\n",
        "    net.train()\n",
        "    losses = []\n",
        "    for data in tqdm.tqdm(loader):\n",
        "        # xははじめから最後の手前の文字まで\n",
        "        x = data[:, :-1]\n",
        "        # yは2文字目から最後の文字まで\n",
        "        y = data[:, 1:]\n",
        "\n",
        "        x = x.to(\"cuda:0\")\n",
        "        y = y.to(\"cuda:0\")\n",
        "\n",
        "        y_pred, _ = net(x)\n",
        "        # batchとstepの軸を統合してからCrossEntropyLossに渡す\n",
        "        loss = loss_f(y_pred.view(-1, vocab_size), y.view(-1))\n",
        "        net.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        losses.append(loss.item())\n",
        "    # 現在の損失関数と生成される文章の例を表示\n",
        "    print(epoch, mean(losses))\n",
        "    with torch.no_grad():\n",
        "        print(generate_seq(net, device=\"cuda:0\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ALG_PSW7CdF-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Colaboratory における圧縮ファイルの展開"
      ]
    },
    {
      "metadata": {
        "id": "MjcobBbirKfE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://www.manythings.org/anki/spa-eng.zip\n",
        "!unzip spa-eng.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QFaGNnORnrL9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.17　補助関数の作成"
      ]
    },
    {
      "metadata": {
        "id": "9Zx9IltZnyIF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import (Dataset, \n",
        "                              DataLoader,\n",
        "                              TensorDataset)\n",
        "import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o5C2qvbRoIoi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import collections\n",
        "import itertools\n",
        "\n",
        "remove_marks_regex = re.compile(\n",
        "    \"[\\,\\(\\)\\[\\]\\*:;¿¡]|<.*?>\")\n",
        "shift_marks_regex = re.compile(\"([?!\\.])\")\n",
        "\n",
        "unk = 0\n",
        "sos = 1\n",
        "eos = 2\n",
        "\n",
        "def normalize(text):\n",
        "    text = text.lower()\n",
        "    # 不要な文字を除去\n",
        "    text = remove_marks_regex.sub(\"\", text)\n",
        "    # ?!.と単語の間に空白を挿入\n",
        "    text = shift_marks_regex.sub(r\" \\1\", text)\n",
        "    return text\n",
        "\n",
        "def parse_line(line):\n",
        "    line = normalize(line.strip())\n",
        "    # 翻訳元(src)と翻訳先(trg)それぞれのトークンのリストを作る\n",
        "    src, trg = line.split(\"\\t\")\n",
        "    src_tokens = src.strip().split()\n",
        "    trg_tokens = trg.strip().split()\n",
        "    return src_tokens, trg_tokens\n",
        "\n",
        "def build_vocab(tokens):\n",
        "    # ファイル中のすべての文章でのトークンの出現数を数える\n",
        "    counts = collections.Counter(tokens)\n",
        "    # トークンの出現数の多い順に並べる\n",
        "    sorted_counts = sorted(counts.items(), \n",
        "                           key=lambda c: c[1], reverse=True)\n",
        "    # 3つのタグを追加して正引きリストと逆引き用辞書を作る\n",
        "    word_list = [\"<UNK>\", \"<SOS>\", \"<EOS>\"] \\\n",
        "        + [x[0] for x in sorted_counts]\n",
        "    word_dict = dict((w, i) for i, w in enumerate(word_list))\n",
        "    return word_list, word_dict\n",
        "    \n",
        "def words2tensor(words, word_dict, max_len, padding=0):\n",
        "    # 末尾に終了タグを付ける\n",
        "    words = words + [\"<EOS>\"]\n",
        "    # 辞書を利用して数値のリストに変換する\n",
        "    words = [word_dict.get(w, 0) for w in words]\n",
        "    seq_len = len(words)\n",
        "    # 長さがmax_len以下の場合はパディングする\n",
        "    if seq_len < max_len + 1:\n",
        "        words = words + [padding] * (max_len + 1 - seq_len)\n",
        "    # Tensorに変換して返す\n",
        "    return torch.tensor(words, dtype=torch.int64), seq_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mccOe_8EoA59",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.18　TranslationPairDatasetクラスの作成"
      ]
    },
    {
      "metadata": {
        "id": "kKU4mNperk9Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TranslationPairDataset(Dataset):\n",
        "    def __init__(self, path, max_len=15):\n",
        "        # 単語数が多い文章をフィルタリングする関数\n",
        "        def filter_pair(p):\n",
        "            return not (len(p[0]) > max_len \n",
        "                        or len(p[1]) > max_len)\n",
        "        # ファイルを開き、パース/フィルタリングをする       \n",
        "        with open(path) as fp:\n",
        "            pairs = map(parse_line, fp)\n",
        "            pairs = filter(filter_pair, pairs)\n",
        "            pairs = list(pairs)\n",
        "        # 文章のペアをソースとターゲットに分ける\n",
        "        src = [p[0] for p in pairs]\n",
        "        trg = [p[1] for p in pairs]\n",
        "        #それぞれの語彙集を作成する\n",
        "        self.src_word_list, self.src_word_dict = \\\n",
        "            build_vocab(itertools.chain.from_iterable(src))\n",
        "        self.trg_word_list, self.trg_word_dict = \\\n",
        "            build_vocab(itertools.chain.from_iterable(trg))\n",
        "        # 語彙集を使用してTensorに変換する\n",
        "        self.src_data = [words2tensor(\n",
        "            words, self.src_word_dict, max_len)\n",
        "                for words in src]\n",
        "        self.trg_data = [words2tensor(\n",
        "            words, self.trg_word_dict, max_len, -100)\n",
        "                for words in trg]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.src_data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        src, lsrc = self.src_data[idx]\n",
        "        trg, ltrg = self.trg_data[idx]\n",
        "        return src, lsrc, trg, ltrg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OXOyt0jwoMnP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.19　DatasetとDataLoaderの作成（<your_path>を変更している）"
      ]
    },
    {
      "metadata": {
        "id": "UguehAH_sv_G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "max_len = 10\n",
        "path = \"spa.txt\"\n",
        "ds = TranslationPairDataset(path, max_len=max_len)\n",
        "loader = DataLoader(ds, batch_size=batch_size, shuffle=True,\n",
        "                    num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "flT7AwtWoW2m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.20　Encoderの作成"
      ]
    },
    {
      "metadata": {
        "id": "Y4La7cmCtA67",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_embeddings,\n",
        "                 embedding_dim=50, \n",
        "                 hidden_size=50,\n",
        "                 num_layers=1,\n",
        "                 dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(num_embeddings, embedding_dim,\n",
        "                                padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                            hidden_size, num_layers,\n",
        "                            batch_first=True, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, h0=None, l=None):\n",
        "        x = self.emb(x)\n",
        "        if l is not None:\n",
        "            x = nn.utils.rnn.pack_padded_sequence(\n",
        "                x, l, batch_first=True)\n",
        "        _, h = self.lstm(x, h0)\n",
        "        return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cyC-WhMzofVq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.21　Decoderの作成（要再チェック）"
      ]
    },
    {
      "metadata": {
        "id": "wxzDeTvetXKL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_embeddings,\n",
        "                 embedding_dim=50, \n",
        "                 hidden_size=50,\n",
        "                 num_layers=1,\n",
        "                 dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(num_embeddings, embedding_dim,\n",
        "                                padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size,\n",
        "                            num_layers, batch_first=True,\n",
        "                            dropout=dropout)\n",
        "        self.linear = nn.Linear(hidden_size, num_embeddings)\n",
        "    def forward(self, x, h, l=None):\n",
        "        x = self.emb(x)\n",
        "        if l is not None:\n",
        "            x = nn.utils.rnn.pack_padded_sequence(\n",
        "                x, l, batch_first=True)\n",
        "        x, h = self.lstm(x, h)\n",
        "        if l is not None:\n",
        "            x = nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0)[0]\n",
        "        x = self.linear(x)\n",
        "        return x, h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_W392HxopBqp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.22　翻訳する関数の作成"
      ]
    },
    {
      "metadata": {
        "id": "UvGlLeGZsJbC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def translate(input_str, enc, dec, max_len=15, device=\"cpu\"):\n",
        "    # 入力文字列を数値化してTensorに変換\n",
        "    words = normalize(input_str).split()\n",
        "    input_tensor, seq_len = words2tensor(words, \n",
        "        ds.src_word_dict, max_len=max_len)\n",
        "    input_tensor = input_tensor.unsqueeze(0)\n",
        "    # Encoderで使用するので入力の長さもリストにしておく\n",
        "    seq_len = [seq_len]\n",
        "    # 開始トークンを準備\n",
        "    sos_inputs = torch.tensor(sos, dtype=torch.int64)\n",
        "    input_tensor = input_tensor.to(device)\n",
        "    sos_inputs = sos_inputs.to(device)\n",
        "    # 入力文字列をEncoderに入れてコンテキストを得る\n",
        "    ctx = enc(input_tensor, l=seq_len)\n",
        "    # 開始トークンとコンテキストをDecoderの初期値にセット\n",
        "    z = sos_inputs\n",
        "    h = ctx\n",
        "    results = []\n",
        "    for i in range(max_len):\n",
        "        # Decoderで次の単語を予測\n",
        "        o, h = dec(z.view(1, 1), h)\n",
        "        # 線形層の出力が最も大きい場所が次の単語のID\n",
        "        wi = o.detach().view(-1).max(0)[1]\n",
        "        if wi.item() == eos:\n",
        "            break\n",
        "        results.append(wi.item())\n",
        "        # 次の入力は今回の出力のIDを使用する\n",
        "        z = wi\n",
        "    # 記録しておいた出力のIDを文字列に変換\n",
        "    return \" \".join(ds.trg_word_list[i] for i in results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XolLZbJ8pJ0p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.23　関数の動作の確認"
      ]
    },
    {
      "metadata": {
        "id": "1FjQG5AWuTnK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "enc = Encoder(len(ds.src_word_list), 100, 100, 2)\n",
        "dec = Decoder(len(ds.trg_word_list), 100, 100, 2)\n",
        "translate(\"I am a student.\", enc, dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BuRB5hgepMX1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.24　オプティマイザーのパラメータ"
      ]
    },
    {
      "metadata": {
        "id": "RF0da0TBueeb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "enc = Encoder(len(ds.src_word_list), 100, 100, 2)\n",
        "dec = Decoder(len(ds.trg_word_list), 100, 100, 2)\n",
        "enc.to(\"cuda:0\")\n",
        "dec.to(\"cuda:0\")\n",
        "opt_enc = optim.Adam(enc.parameters(), 0.002)\n",
        "opt_dec = optim.Adam(dec.parameters(), 0.002)\n",
        "loss_f = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SDmwNm0kpSnt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リスト5.25　モデルの学習部分（損失関数など）"
      ]
    },
    {
      "metadata": {
        "id": "d3e8_A3fwJEM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "\n",
        "def to2D(x):\n",
        "    shapes = x.shape\n",
        "    return x.reshape(shapes[0] * shapes[1], -1)\n",
        "\n",
        "for epoc in range(30):\n",
        "    # ネットワークを訓練モードにする\n",
        "    enc.train(), dec.train()\n",
        "    losses = []\n",
        "    for x, lx, y, ly in tqdm.tqdm(loader):\n",
        "        # xのPackedSequenceを作るために翻訳元の長さで降順にソート\n",
        "        lx, sort_idx = lx.sort(descending=True)\n",
        "        x, y, ly = x[sort_idx], y[sort_idx], ly[sort_idx]\n",
        "        x, y = x.to(\"cuda:0\"), y.to(\"cuda:0\")\n",
        "        # 翻訳元をEncoderに入れてコンテキストを得る\n",
        "        ctx = enc(x, l=lx)\n",
        "        \n",
        "        # yのPackedSequenceを作るために翻訳先の長さで降順にソート\n",
        "        ly, sort_idx = ly.sort(descending=True)\n",
        "        y = y[sort_idx]\n",
        "        # Decoderの初期値をセット\n",
        "        h0 = (ctx[0][:, sort_idx, :], ctx[1][:, sort_idx, :])\n",
        "        z = y[:, :-1].detach()\n",
        "        # -100のままだとEmbeddingの計算でエラーが出てしまうので値を0に変更しておく\n",
        "        z[z==-100] = 0\n",
        "        # Decoderに通して損失関数を計算\n",
        "        o, _ = dec(z, h0, l=ly-1)\n",
        "        loss = loss_f(to2D(o[:]), to2D(y[:, 1:max(ly)]).squeeze())\n",
        "        # Backpropagation（誤差逆伝播法）を実行\n",
        "        enc.zero_grad(), dec.zero_grad()\n",
        "        loss.backward()\n",
        "        opt_enc.step(), opt_dec.step()\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "    # データセットに対して一通り計算したら現在の\n",
        "    # 損失関数の値や翻訳結果を表示\n",
        "    enc.eval(), dec.eval()\n",
        "    print(epoc, mean(losses))\n",
        "    with torch.no_grad():\n",
        "        print(translate(\"I am a student.\",\n",
        "                         enc, dec, max_len=max_len, device=\"cuda:0\"))\n",
        "        print(translate(\"He likes to eat pizza.\",\n",
        "                         enc, dec, max_len=max_len, device=\"cuda:0\"))\n",
        "        print(translate(\"She is my mother.\",\n",
        "                         enc, dec, max_len=max_len, device=\"cuda:0\"))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}