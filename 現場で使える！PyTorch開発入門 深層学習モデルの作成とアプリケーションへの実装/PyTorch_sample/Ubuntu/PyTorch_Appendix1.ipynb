{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch_Appendix1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "cYW1guETgPZY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Appendix 1　訓練の様子を可視化する"
      ]
    },
    {
      "metadata": {
        "id": "68JjgeNQigoX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リストA1.1　train_net関数の作成"
      ]
    },
    {
      "metadata": {
        "id": "XPvfLSWjgPZZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision import transforms\n",
        "\n",
        "# 訓練用のデータを取得\n",
        "# そのままだとPILの画像形式でDatasetを作ってしまうので\n",
        "# transforms.ToTensorでTensorに変換する\n",
        "fashion_mnist_train = FashionMNIST(\"<your_path>/FashionMNIST\", \n",
        "    train=True, download=True,\n",
        "    transform=transforms.ToTensor())\n",
        "# 検証用データの取得\n",
        "fashion_mnist_test = FashionMNIST(\"<your_path>/FashionMNIST\",\n",
        "    train=False, download=True,\n",
        "    transform=transforms.ToTensor())\n",
        "\n",
        "# バッチサイズが128のDataLoaderをそれぞれ作成\n",
        "batch_size=128\n",
        "train_loader = DataLoader(fashion_mnist_train, \n",
        "                          batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(fashion_mnist_test,\n",
        "                         batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mNguKu19gPZb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# (N, C, H, W)形式のTensorを(N, C*H*W)に引き伸ばす層\n",
        "# 畳み込み層の出力をMLPに渡す際に必要\n",
        "class FlattenLayer(nn.Module):\n",
        "    def forward(self, x):\n",
        "        sizes = x.size()\n",
        "        return x.view(sizes[0], -1)\n",
        "\n",
        "# 5x5のカーネルを使用し最初に32つ、次の64つのチャンネルを作成する\n",
        "# BatchNorm2dは画像形式用のBatch Normalization\n",
        "# Dropout2dは画像形式用のDropout\n",
        "# 最後にFlattenLayerを挟む\n",
        "conv_net = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, 5),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.Dropout2d(0.25),\n",
        "    nn.Conv2d(32, 64, 5),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.Dropout2d(0.25),\n",
        "    FlattenLayer()\n",
        ")\n",
        "\n",
        "# 畳み込みによって最終的にどのようなサイズになっているかを\n",
        "# 実際にデータを入れてみて確認する\n",
        "test_input = torch.ones(1, 1, 28, 28)\n",
        "conv_output_size = conv_net(test_input).size()[-1]\n",
        "\n",
        "# 2層のMLP\n",
        "mlp = nn.Sequential(\n",
        "    nn.Linear(conv_output_size, 200),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm1d(200),\n",
        "    nn.Dropout(0.25),\n",
        "    nn.Linear(200, 10)\n",
        ")\n",
        "\n",
        "# 最終的なCNN\n",
        "net = nn.Sequential(\n",
        "    conv_net,\n",
        "    mlp\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b-v9n3DygPZh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 評価のヘルパー関数\n",
        "def eval_net(net, data_loader, device=\"cpu\"):\n",
        "    # DropoutやBatchNormを無効化\n",
        "    net.eval()\n",
        "    ys = []\n",
        "    ypreds = []\n",
        "    for x, y in data_loader:\n",
        "        # toメソッドで計算を実行するデバイスに転送する\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        # 確率が最大のクラスを予測。(3章参照)\n",
        "        # ここではfowardの計算だけなので自動部分に\n",
        "        # 必要な余計な処理はoffにする\n",
        "        with torch.no_grad():\n",
        "            _, y_pred = net(x).max(1)\n",
        "        ys.append(y)\n",
        "        ypreds.append(y_pred)\n",
        "    # ミニバッチごとの予測結果などを1つにまとめる\n",
        "    ys = torch.cat(ys)\n",
        "    ypreds = torch.cat(ypreds)\n",
        "    # 予測精度を計算\n",
        "    acc = (ys == ypreds).float().sum() / len(ys)\n",
        "    return acc.item()\n",
        "\n",
        "# 訓練のヘルパー関数\n",
        "def train_net(net, train_loader, test_loader,\n",
        "              optimizer_cls=optim.Adam,\n",
        "              loss_fn=nn.CrossEntropyLoss(),\n",
        "              n_iter=10, device=\"cpu\", writer=None):\n",
        "    train_losses = []\n",
        "    train_acc = []\n",
        "    val_acc = []\n",
        "    optimizer = optimizer_cls(net.parameters())\n",
        "    for epoch in range(n_iter):\n",
        "        running_loss = 0.0\n",
        "        # ネットワークを訓練モードにする\n",
        "        net.train()\n",
        "        n = 0\n",
        "        n_acc = 0\n",
        "        # 非常に時間がかかるのでtqdmを使用してプログレスバーを出す\n",
        "        for i, (xx, yy) in tqdm.tqdm(enumerate(train_loader),\n",
        "            total=len(train_loader)):\n",
        "            xx = xx.to(device)\n",
        "            yy = yy.to(device)\n",
        "            h = net(xx)\n",
        "            loss = loss_fn(h, yy)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            n += len(xx)\n",
        "            _, y_pred = h.max(1)\n",
        "            n_acc += (yy == y_pred).float().sum().item()\n",
        "        train_losses.append(running_loss / i)\n",
        "        # 訓練データの予測精度\n",
        "        train_acc.append(n_acc / n)\n",
        "        # 検証データの予測精度\n",
        "        val_acc.append(eval_net(net, test_loader, device))\n",
        "        # このepochでの結果を表示\n",
        "        print(epoch, train_losses[-1], train_acc[-1], val_acc[-1], flush=True)\n",
        "        if writer is not None:\n",
        "            writer.add_scalar('train_loss', train_losses[-1], epoch)\n",
        "            writer.add_scalars('accuracy', {\n",
        "            \"train\": train_acc[-1],\n",
        "            \"validation\": val_acc[-1]\n",
        "            }, epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vg7IW4Nki1sQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "リストA1.2　ログの出力"
      ]
    },
    {
      "metadata": {
        "id": "07fRlmKUgPZj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorboardX import  SummaryWriter\n",
        "\n",
        "# SummaryWriter を作成\n",
        "writer = SummaryWriter(\"/tmp/cnn\")\n",
        "\n",
        "# 訓練を実行\n",
        "net.to(\"cuda:0\")\n",
        "train_net(net, train_loader, test_loader, n_iter=20, device=\"cuda:0\", writer=writer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}